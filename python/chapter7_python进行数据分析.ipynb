{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-1 机器学习基础\n",
    "机器学习是让计算机有学习能力, 主要分为有监督学习和无监督学习\n",
    "-   有监督学习\n",
    "    -   有监督学习研究的是问题存在正确答案\n",
    "-   无监督学习\n",
    "    -   问题需要尝试\n",
    "-   强化学习\n",
    "    -   给定条件下寻找最大的回报行为\n",
    "-   规则学习\n",
    "    -   按照人们给定的规则预测结果\n",
    "\n",
    "## 机器学习和统计学的关系\n",
    "\n",
    "机器学习和统计学有着密切的关系。机器学习可以看作是统计学和计算机科学的结合，它在理论研究方面本质上还是以统计学的理论为核心，而在应用领域则更偏重于计算机的实现1。统计学是模型驱动的，而机器学习则是算法驱动的1。机器学习强调大规模应用和预测准确性，而统计学习则强调模型及其可解释性、精度和不确定性2。\n",
    ">[论统计学与机器学习的关系与发展前景](https://blog.csdn.net/ajksunke/article/details/53038457)\n",
    "\n",
    ">[机器学习和统计学习的区别：10个统计分析方法](https://blog.csdn.net/ajksunke/article/details/53038457)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化、Ridge回归与Lasso回归\n",
    "正则化、Ridge回归和Lasso回归都是机器学习中用于解决过拟合问题的常见技术。\n",
    "\n",
    "在参数估计中，像损失函数引入惩罚指标以防系数过大的措施叫正则化。在线性回归中，正则化项通常是模型参数的平方和或绝对值之和。正则化可以有效地减少模型的过拟合，提高模型的泛化能力。\n",
    "\n",
    "正则化是一种用于防止过拟合的技术。过拟合是指模型在训练数据上表现得很好，但在新数据上表现不佳的情况。这通常是由于模型过于复杂，以至于它能够“记住”训练数据中的噪声和异常值，而不是学习到真正有用的信息。\n",
    "\n",
    "正则化通过在损失函数中添加一个正则化项来限制模型的复杂度，从而防止过拟合。这个正则化项通常与模型参数有关，它会惩罚模型参数过大或过小的情况。这样，模型就不能随意地拟合训练数据中的噪声和异常值，而只能学习到真正有用的信息。\n",
    "\n",
    "常见的正则化方法包括L1正则化和L2正则化。它们分别通过在损失函数中添加模型参数的绝对值之和和平方和作为正则化项来实现。这两种方法都可以有效地防止过拟合，但它们之间也有一些区别。\n",
    "\n",
    "\n",
    "Ridge回归和Lasso回归都是基于正则化的线性回归方法。Ridge回归是通过添加L2正则化项来限制模型参数的平方和，而Lasso回归则是通过添加L1正则化项来限制模型参数的绝对值之和。相比于普通的线性回归，Ridge回归和Lasso回归可以在不降低模型预测能力的情况下，有效地缩小模型参数的范围，避免过拟合。\n",
    "\n",
    "Ridge回归和Lasso回归的不同之处在于，Lasso回归会将一部分参数缩小至零，从而实现特征选择的效果，而Ridge回归则会将参数缩小但不会将其缩小至零。因此，在应用Ridge回归和Lasso回归时，需要根据具体问题选择合适的方法。\n",
    "\n",
    "总之，正则化、Ridge回归和Lasso回归是机器学习中重要的技术，可以帮助我们解决过拟合问题，提高模型的泛化能力和预测性能。\n",
    "\n",
    "### 例子\n",
    "Ridge回归和Lasso回归都是正则化线性回归的方法，它们的数学公式如下：\n",
    "\n",
    "对于普通的线性回归，假设有 $n$ 个样本和 $p$ 个特征，用 $X \\in \\mathbb{R}^{n \\times p}$ 表示特征矩阵，$y \\in \\mathbb{R}^n$ 表示响应变量向量。线性回归的目标是找到一个参数向量 $\\beta \\in \\mathbb{R}^p$ 使得预测值 $\\hat{y} = X\\beta$ 最小化残差平方和：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2$$\n",
    "\n",
    "在Ridge回归和Lasso回归中，我们添加一个正则化项来限制参数向量的大小。Ridge回归使用L2正则化项，Lasso回归使用L1正则化项，因此它们的目标函数如下：\n",
    "\n",
    "Ridge回归：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2 + \\alpha |\\beta|^2$$\n",
    "\n",
    "其中，$\\alpha$ 是正则化力度，可以控制参数向量的大小。L2正则化项 $\\alpha \\|\\beta\\|^2$ 会使参数向量的大小缩小，但不会使其中任何一个参数缩小至零。\n",
    "\n",
    "Lasso回归：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2 + \\alpha |\\beta|$$\n",
    "\n",
    "其中，L1正则化项 $\\alpha |\\beta|$ 会使部分参数缩小至零，从而实现特征选择的效果。这是因为L1正则化项具有稀疏性，可以将一些不重要的特征的系数缩小至零。\n",
    "\n",
    "在实际应用中，我们需要通过交叉验证等方法来选择合适的正则化参数 $\\alpha$，以平衡模型的拟合能力和泛化能力。\n",
    "\n",
    ">[正则化是什么？以及Ridge和 Lasso回归的区别](https://zhuanlan.zhihu.com/p/111562068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
