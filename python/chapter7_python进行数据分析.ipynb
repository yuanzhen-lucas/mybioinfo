{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-1 机器学习基础\n",
    "机器学习是让计算机有学习能力, 主要分为有监督学习和无监督学习\n",
    "-   有监督学习\n",
    "    -   有监督学习研究的是问题存在正确答案\n",
    "-   无监督学习\n",
    "    -   问题需要尝试\n",
    "-   强化学习\n",
    "    -   给定条件下寻找最大的回报行为\n",
    "-   规则学习\n",
    "    -   按照人们给定的规则预测结果\n",
    "\n",
    "## 机器学习和统计学的关系\n",
    "\n",
    "机器学习和统计学有着密切的关系。机器学习可以看作是统计学和计算机科学的结合，它在理论研究方面本质上还是以统计学的理论为核心，而在应用领域则更偏重于计算机的实现1。统计学是模型驱动的，而机器学习则是算法驱动的1。机器学习强调大规模应用和预测准确性，而统计学习则强调模型及其可解释性、精度和不确定性2。\n",
    ">[论统计学与机器学习的关系与发展前景](https://blog.csdn.net/ajksunke/article/details/53038457)\n",
    "\n",
    ">[机器学习和统计学习的区别：10个统计分析方法](https://blog.csdn.net/ajksunke/article/details/53038457)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化、Ridge回归与Lasso回归\n",
    "正则化、Ridge回归和Lasso回归都是机器学习中用于解决过拟合问题的常见技术。\n",
    "\n",
    "在参数估计中，像损失函数引入惩罚指标以防系数过大的措施叫正则化。在线性回归中，正则化项通常是模型参数的平方和或绝对值之和。正则化可以有效地减少模型的过拟合，提高模型的泛化能力。\n",
    "\n",
    "正则化是一种用于防止过拟合的技术。过拟合是指模型在训练数据上表现得很好，但在新数据上表现不佳的情况。这通常是由于模型过于复杂，以至于它能够“记住”训练数据中的噪声和异常值，而不是学习到真正有用的信息。\n",
    "\n",
    "正则化通过在损失函数中添加一个正则化项来限制模型的复杂度，从而防止过拟合。这个正则化项通常与模型参数有关，它会惩罚模型参数过大或过小的情况。这样，模型就不能随意地拟合训练数据中的噪声和异常值，而只能学习到真正有用的信息。\n",
    "\n",
    "常见的正则化方法包括L1正则化和L2正则化。它们分别通过在损失函数中添加模型参数的绝对值之和和平方和作为正则化项来实现。这两种方法都可以有效地防止过拟合，但它们之间也有一些区别。\n",
    "\n",
    "\n",
    "Ridge回归和Lasso回归都是基于正则化的线性回归方法。Ridge回归是通过添加L2正则化项来限制模型参数的平方和，而Lasso回归则是通过添加L1正则化项来限制模型参数的绝对值之和。相比于普通的线性回归，Ridge回归和Lasso回归可以在不降低模型预测能力的情况下，有效地缩小模型参数的范围，避免过拟合。\n",
    "\n",
    "Ridge回归和Lasso回归的不同之处在于，Lasso回归会将一部分参数缩小至零，从而实现特征选择的效果，而Ridge回归则会将参数缩小但不会将其缩小至零。因此，在应用Ridge回归和Lasso回归时，需要根据具体问题选择合适的方法。\n",
    "\n",
    "总之，正则化、Ridge回归和Lasso回归是机器学习中重要的技术，可以帮助我们解决过拟合问题，提高模型的泛化能力和预测性能。\n",
    "\n",
    "### 例子\n",
    "Ridge回归和Lasso回归都是正则化线性回归的方法，它们的数学公式如下：\n",
    "\n",
    "对于普通的线性回归，假设有 $n$ 个样本和 $p$ 个特征，用 $X \\in \\mathbb{R}^{n \\times p}$ 表示特征矩阵，$y \\in \\mathbb{R}^n$ 表示响应变量向量。线性回归的目标是找到一个参数向量 $\\beta \\in \\mathbb{R}^p$ 使得预测值 $\\hat{y} = X\\beta$ 最小化残差平方和：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2$$\n",
    "\n",
    "在Ridge回归和Lasso回归中，我们添加一个正则化项来限制参数向量的大小。Ridge回归使用L2正则化项，Lasso回归使用L1正则化项，因此它们的目标函数如下：\n",
    "\n",
    "Ridge回归：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2 + \\alpha |\\beta|^2$$\n",
    "\n",
    "其中，$\\alpha$ 是正则化力度，可以控制参数向量的大小。L2正则化项 $\\alpha \\|\\beta\\|^2$ 会使参数向量的大小缩小，但不会使其中任何一个参数缩小至零。\n",
    "\n",
    "Lasso回归：\n",
    "\n",
    "$$\\min_{\\beta} |y - X\\beta|^2 + \\alpha |\\beta|$$\n",
    "\n",
    "其中，L1正则化项 $\\alpha |\\beta|$ 会使部分参数缩小至零，从而实现特征选择的效果。这是因为L1正则化项具有稀疏性，可以将一些不重要的特征的系数缩小至零。\n",
    "\n",
    "在实际应用中，我们需要通过交叉验证等方法来选择合适的正则化参数 $\\alpha$，以平衡模型的拟合能力和泛化能力。\n",
    "\n",
    ">[正则化是什么？以及Ridge和 Lasso回归的区别](https://zhuanlan.zhihu.com/p/111562068)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定正则化强度\n",
    "正则化强度是一个超参数(α),可以通过交叉验证来确定。在交叉验证过程中，可以尝试不同的正则化强度值，最后选取预测精度最高的值。这个过程可以通过网格搜索或随机搜索来实现。\n",
    "\n",
    "交叉验证是一种评估模型泛化能力的方法。它通过将数据集分成若干份，然后在其中一份上训练模型，在另一份上测试模型的性能，来评估模型的泛化能力。这个过程可以重复多次，每次使用不同的数据划分，最后取所有测试结果的平均值作为模型的最终评估结果。常见的交叉验证方法有 k 折交叉验证和留一交叉验证。\n",
    "\n",
    "### 解释变量标准化\n",
    "在进行Ridge回归或Lasso回归之前,应当将解释变量标准化，即让解释变量的均值为0，标准差为1。\n",
    "在机器学习中，解释变量标准化是一个常见的预处理步骤，其目的是将解释变量（也称为特征）缩放到统一的范围。这是因为，不同的特征通常具有不同的尺度和范围，这可能会导致某些特征在模型训练中对结果的影响更大，从而影响模型的性能。\n",
    "\n",
    "通过标准化，我们可以将所有的特征缩放到同一尺度，消除不同特征之间的量纲影响，使得模型中的每个特征都可以对模型的结果产生相同的影响。这样可以更好地比较不同特征对模型预测的贡献，提高模型的性能和稳定性。\n",
    "\n",
    "此外，标准化还可以帮助我们处理异常值和离群值，提高模型的鲁棒性和准确性。\n",
    "\n",
    "### 变量选择与正则化的对比\n",
    "变量选择（如AIC最小准则）会从模型中去除不必要的解释变量，从而减少需要估计的参数个数，简化模型，以便更好地避免出现过拟合的问题。\n",
    "另外一种避免过拟合的方法就是正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
